{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e54a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fed83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function find_jpg_files searches for all .jpg files within a specified directory and its subdirectories. \n",
    "#It returns a list containing the paths of these .jpg files.\n",
    "\n",
    "def find_jpg_files(search_dir):\n",
    "    \"\"\"\n",
    "    Find all .jpg files within the given directory and its subdirectories.\n",
    "\n",
    "    Parameters:\n",
    "    - search_dir: The path of the directory to search in.\n",
    "\n",
    "    Returns:\n",
    "    - A list of paths to .jpg files found within the specified directory and its subdirectories.\n",
    "    \"\"\"\n",
    "    jpg_files = []  # List to hold the paths of .jpg files\n",
    "\n",
    "    # Walk through the directory and its subdirectories\n",
    "    for root, dirs, files in os.walk(search_dir):\n",
    "        for file in files:\n",
    "            # Check if the file ends with .jpg\n",
    "            if file.endswith('.jpg'):\n",
    "                # Construct the full path and add it to the list\n",
    "                full_path = os.path.join(root, file)\n",
    "                jpg_files.append(full_path)\n",
    "                \n",
    "    return jpg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function autocropmin performs cropping of an image based on a minimum intensity threshold. \n",
    "#It reduces the image size by removing regions that are below a specified intensity threshold, \n",
    "#likely focusing on a central object or area of interest. \n",
    "\n",
    "def autocropmin(image, threshold=100, kernsel_size = 10):\n",
    "        \n",
    "    img = image.copy()\n",
    "    \n",
    "    SIZE = img.shape[0]\n",
    "    # Apply a minimum filter to the image with the specified kernel size\n",
    "    imgfilt = ndimage.minimum_filter(img, size=kernsel_size)\n",
    "    \n",
    "    # Binarize the image: set pixels below the threshold to 0, others to 255\n",
    "    img_b=np.where(imgfilt<threshold,0,255)\n",
    "    \n",
    "    \n",
    "    a=img_b[:,:,0].sum(axis=1)\n",
    "    a=np.concatenate(([0],a,[0]))\n",
    "\n",
    "    a_=np.where(a==0)[0]\n",
    "    mina=a_[np.argmax(a_[1:]-a_[:-1])]\n",
    "    maxa=a_[np.argmax(a_[1:]-a_[:-1])+1]-1\n",
    "\n",
    "    b=img_b[:,:,0].sum(axis=0)\n",
    "    b=np.concatenate(([0],b,[0]))\n",
    "\n",
    "    b_=np.where(b==0)[0]\n",
    "    minb=b_[np.argmax(b_[1:]-b_[:-1])]\n",
    "    maxb=b_[np.argmax(b_[1:]-b_[:-1])+1]-1\n",
    "\n",
    "    if  mina!=maxa and minb!=maxb:\n",
    "        # Crop the image to the determined boundaries\n",
    "        imageout=img[mina:maxa,minb:maxb,:]\n",
    "    else:\n",
    "        imageout=img\n",
    "\n",
    "    return imageout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ba2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path of your data\n",
    "\n",
    "# Path of Challenge1 data\n",
    "search_dir = '/mnt/ephemeral/challenge_data/challenge1_data/OriginalDatauncompressed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold the paths of .jpg files of Challenge 1 Dataset\n",
    "jpg_files = find_jpg_files(search_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a3309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(jpg_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648021c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number=0\n",
    "len_jpg=len(jpg_files)\n",
    "\n",
    "#The process_image function processes a given image by reading it, cropping it based on intensity thresholds,\n",
    "#and saving the processed image to a new location\n",
    "\n",
    "def process_image(str1):\n",
    "    img = cv2.imread(str1)\n",
    "    if img is None:\n",
    "        return str1, True, None  # Indicate that the file is a bug file\n",
    "\n",
    "    ct_scan = \"/\".join(str1.split(\"/\")[-6:-1])\n",
    "    new_shape = img.shape  \n",
    "\n",
    "    # Process the image \n",
    "    img = autocropmin(img)\n",
    "\n",
    "    # Prepare the output path\n",
    "    str1 = str1.replace(\"/OriginalDatauncompressed/\", \"/preprocessed/\")\n",
    "    folder_path = \"/\".join(str1.split(\"/\")[:-1])\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    cv2.imwrite(str1, img)\n",
    "    \n",
    "    return ct_scan, False, new_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2334978",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Challenge 1 Dataset ###############################\n",
    "#The following processes the list of .jpg files of the challenge 1 dataset concurrently. \n",
    "##########################################################################################\n",
    "\n",
    "shape_dict = {}\n",
    "diff_shape = []\n",
    "bug_files = []\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit all tasks and wrap them with tqdm for a progress bar\n",
    "    futures = [executor.submit(process_image, str1) for str1 in jpg_files]\n",
    "    \n",
    "    # Use tqdm to wrap the as_completed generator\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing Images\"):\n",
    "        ct_scan, is_bug_file, shape = future.result()\n",
    "        if is_bug_file:\n",
    "            bug_files.append(ct_scan)\n",
    "        else:\n",
    "            if ct_scan not in shape_dict:\n",
    "                shape_dict[ct_scan] = set()\n",
    "            shape_dict[ct_scan].add(shape)\n",
    "            if len(shape_dict[ct_scan]) != 1:\n",
    "                diff_shape.append(ct_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa42ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Challenge 2 dataset##################\n",
    "## Repeating the above but for Challenge 2 dataset######\n",
    "\n",
    "# Path of Challenge2 data\n",
    "search_dir = '/mnt/ephemeral/challenge_data/challenge2_data/dataset'\n",
    "\n",
    "# List to hold the paths of .jpg files\n",
    "jpg_files = find_jpg_files(search_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25db2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number=0\n",
    "len_jpg=len(jpg_files)\n",
    "\n",
    "def process_image(str1):\n",
    "    img = cv2.imread(str1)\n",
    "    if img is None:\n",
    "        return str1, True, None  # Indicate that the file is a bug file\n",
    "\n",
    "    ct_scan = \"/\".join(str1.split(\"/\")[-6:-1])\n",
    "    new_shape = img.shape  \n",
    "\n",
    "    # Process the image \n",
    "    img = autocropmin(img)\n",
    "\n",
    "    # Prepare the output path\n",
    "    str1 = str1.replace(\"/dataset/\", \"/preprocessed/\")\n",
    "    folder_path = \"/\".join(str1.split(\"/\")[:-1])\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    cv2.imwrite(str1, img)\n",
    "    \n",
    "    return ct_scan, False, new_shape\n",
    "\n",
    "\n",
    "shape_dict = {}\n",
    "diff_shape = []\n",
    "bug_files = []\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit all tasks and wrap them with tqdm for a progress bar\n",
    "    futures = [executor.submit(process_image, str1) for str1 in jpg_files]\n",
    "    \n",
    "    # Use tqdm to wrap the as_completed generator\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing Images\"):\n",
    "        ct_scan, is_bug_file, shape = future.result()\n",
    "        if is_bug_file:\n",
    "            bug_files.append(ct_scan)\n",
    "        else:\n",
    "            if ct_scan not in shape_dict:\n",
    "                shape_dict[ct_scan] = set()\n",
    "            shape_dict[ct_scan].add(shape)\n",
    "            if len(shape_dict[ct_scan]) != 1:\n",
    "                diff_shape.append(ct_scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b108d0a",
   "metadata": {},
   "source": [
    "######   The following for computing the lung area in each slice in Challenge1 Dataset  #######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0bbf9b",
   "metadata": {},
   "source": [
    "# Get Area of Each Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3400dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dir = '/mnt/ephemeral/challenge_data/challenge1_data/preprocessed'\n",
    "\n",
    "challenge1_preprocessed=find_jpg_files(search_dir)\n",
    "print(len(challenge1_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to computer area of a single image\n",
    "def process_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    img2 = ndimage.minimum_filter(img, 5)\n",
    "    img_b = np.where(img2 < 100, 0, 255)\n",
    "    mask = scipy.ndimage.binary_fill_holes(img_b[:, :, 0])\n",
    "    mask_ = mask * 255\n",
    "    aaa = mask_ - img_b[:, :, 0]\n",
    "    area = aaa.sum() / 255\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf97e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Compute the area using multi threading ########\n",
    "def process_images_multithreaded_ordered(image_paths):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Directly convert executor.map to list without tqdm\n",
    "        results = list(executor.map(process_image, image_paths))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19281170",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge1_train_area = process_images_multithreaded_ordered(challenge1_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4beb3e6",
   "metadata": {},
   "source": [
    "######   The following for computing the lung area in each slice in Challenge2 Dataset  #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732bc533",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_dir = '/mnt/ephemeral/challenge_data/challenge2_data/preprocessed'\n",
    "\n",
    "challenge2_preprocessed=find_jpg_files(search_dir)\n",
    "print(len(challenge2_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dbc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_train_area = process_images_multithreaded_ordered(challenge2_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0341146c",
   "metadata": {},
   "source": [
    "# Saving the processed information ################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4999e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge1_preprocessed_train_indices = [i for i, path in enumerate(challenge1_preprocessed) if 'train' in path]\n",
    "challenge1_preprocessed_valid_indices = [i for i, path in enumerate(challenge1_preprocessed) if 'valid' in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'train' paths using challenge1_preprocessed_train_indices\n",
    "challenge1_preprocessed_train_paths = [challenge1_preprocessed[i] for i in challenge1_preprocessed_train_indices]\n",
    "\n",
    "# Extract corresponding 'train' areas using the same indices\n",
    "challenge1_train_areas = [challenge1_train_area[i] for i in challenge1_preprocessed_train_indices]\n",
    "\n",
    "# Extract 'valid' paths using challenge1_preprocessed_valid_indices\n",
    "challenge1_preprocessed_valid_paths = [challenge1_preprocessed[i] for i in challenge1_preprocessed_valid_indices]\n",
    "\n",
    "# Extract corresponding 'valid' areas using the same indices\n",
    "challenge1_valid_areas = [challenge1_train_area[i] for i in challenge1_preprocessed_valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddb86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge1_train_path_area=pd.DataFrame((zip(challenge1_preprocessed_train_paths, challenge1_train_areas)), columns = ['path', 'area'])\n",
    "challenge1_valid_path_area=pd.DataFrame((zip(challenge1_preprocessed_valid_paths, challenge1_valid_areas)), columns = ['path', 'area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcefd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files='/mnt/challenge_preprocessing/files'\n",
    "challenge1_train_path_area.to_csv(os.path.join(path_to_files,'challenge1_train_path_area.csv'))\n",
    "challenge1_valid_path_area.to_csv(os.path.join(path_to_files,'challenge1_valid_path_area.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_preprocessed_valid_indices = [i for i, path in enumerate(challenge2_preprocessed) if 'Valid' in path]\n",
    "challenge2_indices_with_train_and_annotated = [i for i, path in enumerate(challenge2_preprocessed) if 'Train' in path and '/annotated' in path]\n",
    "challenge2_indices_with_train_and_non_annotated = [i for i, path in enumerate(challenge2_preprocessed) if 'Train' in path and '/non-annotated' in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddfff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_preprocessed_valid_paths = [challenge2_preprocessed[i] for i in challenge2_preprocessed_valid_indices]\n",
    "challenge2_valid_areas = [challenge2_train_area[i] for i in challenge2_preprocessed_valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_preprocessed_train_annotated_paths=[challenge2_preprocessed[i] for i in challenge2_indices_with_train_and_annotated]\n",
    "challenge2_train_annotated_area=  [challenge2_train_area[i] for i in challenge2_indices_with_train_and_annotated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_preprocessed_train_non_annotated_paths=[challenge2_preprocessed[i] for i in challenge2_indices_with_train_and_non_annotated]\n",
    "challenge2_train_non_annotated_area=  [challenge2_train_area[i] for i in challenge2_indices_with_train_and_non_annotated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252e6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_train_path_area=pd.DataFrame((zip(challenge2_preprocessed_train_annotated_paths, challenge2_train_annotated_area)), columns = ['path', 'area'])\n",
    "challenge2_non_annotated_path_area=pd.DataFrame((zip(challenge2_preprocessed_train_non_annotated_paths, challenge2_train_non_annotated_area)), columns = ['path', 'area'])\n",
    "challenge2_valid_path_area=pd.DataFrame((zip(challenge2_preprocessed_valid_paths, challenge2_valid_areas)), columns = ['path', 'area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99826f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_train_path_area.to_csv(os.path.join(path_to_files,'challenge2_train_path_area.csv'))\n",
    "challenge2_valid_path_area.to_csv(os.path.join(path_to_files,'challenge2_valid_path_area.csv'))\n",
    "challenge2_non_annotated_path_area.to_csv(os.path.join(path_to_files,'challenge2_non_annotated_path_area.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd97d2c",
   "metadata": {},
   "source": [
    "# Sort the slices to group the slices of the same scan ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "challenge1_valid_area_path = pd.read_csv(os.path.join(path_to_files, 'challenge1_valid_path_area.csv'))\n",
    "\n",
    "\n",
    "challenge1_valid_area_path[\"ct_path\"] = challenge1_valid_area_path[\"path\"].apply(lambda x: \"/\".join(x.split(\"/\")[:-1]))\n",
    "challenge1_valid_area_path[\"ct_slice\"] = challenge1_valid_area_path[\"path\"].apply(lambda x: int(x.split(\"/\")[-1].split(\".\")[0]))\n",
    "\n",
    "\n",
    "challenge1_valid_area_path.sort_values(by=['ct_path', 'ct_slice'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge1_train_area_path=pd.read_csv(os.path.join(path_to_files,'challenge1_train_path_area.csv'))\n",
    "\n",
    "challenge1_train_area_path[\"ct_path\"]=challenge1_train_area_path[\"path\"].apply(lambda x: \"/\".join(x.split(\"/\")[:-1]))\n",
    "\n",
    "challenge1_train_area_path[\"ct_slice\"]=challenge1_train_area_path[\"path\"].apply(lambda x: int(x.split(\"/\")[-1].split(\".\")[0]))\n",
    "\n",
    "challenge1_train_area_path.sort_values(by=['ct_path', 'ct_slice'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following function finds a contiguous subarray within an array a of length k,\n",
    "#such that the sum of the subarray's elements is maximized. \n",
    "#It returns the starting and ending indices of this subarray.\n",
    "\n",
    "def sum_max(a,w=0.4):\n",
    "    l=len(a)\n",
    "    k=int(np.ceil(l*w))\n",
    "    d=0\n",
    "    tmp_max=0\n",
    "    # print(l, k)\n",
    "    for i in range(l-k+1):\n",
    "        if np.sum(a[i:i+k])>tmp_max:\n",
    "            tmp_max=np.sum(a[i:i+k])\n",
    "            d=i\n",
    "    return d,d+k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Validation of challenge 1 Dataset ############\n",
    "\n",
    "# The following finds and store the subarray within the \"area\" values of each unique CT path \n",
    "#that has the maximum sum for a specified window size (50% of the length of the \"area\" values)\n",
    "\n",
    "challenge1_ct_path_list = challenge1_valid_area_path[\"ct_path\"].unique()\n",
    "challenge1_valid_dic = {}\n",
    "\n",
    "for i in tqdm(range(len(challenge1_ct_path_list))):\n",
    "\n",
    "    tmp_df = challenge1_valid_area_path[challenge1_valid_area_path[\"ct_path\"] == challenge1_ct_path_list[i]].reset_index(drop=True)\n",
    "    challenge1_valid_dic[challenge1_ct_path_list[i]] = list(sum_max(tmp_df[\"area\"].values, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4ec272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training of challenge 1 Dataset ############\n",
    "\n",
    "challenge1_ct_path_list=challenge1_train_area_path[\"ct_path\"].unique()\n",
    "challenge1_train_dic={}\n",
    "for i in tqdm(range(len(challenge1_ct_path_list))):\n",
    "    tmp_df=challenge1_train_area_path[challenge1_train_area_path[\"ct_path\"]==challenge1_ct_path_list[i]].reset_index(drop=True)\n",
    "    challenge1_train_dic[challenge1_ct_path_list[i]]=list(sum_max(tmp_df[\"area\"].values,0.5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912e3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge1_train_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9426c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(challenge1_train_dic),len(challenge1_valid_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba244a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Saving the processed Info. #############\n",
    "\n",
    "with open(os.path.join(path_to_files,'challenge1_valid_range.pickle'), 'wb') as handle:\n",
    "    pickle.dump(challenge1_valid_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_files,'challenge1_train_range.pickle'), 'wb') as handle:\n",
    "    pickle.dump(challenge1_train_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ffe3f9",
   "metadata": {},
   "source": [
    "##### Doing the same for challenge 2 dataset ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef524fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file for 'train' in challenge2\n",
    "challenge2_train_area_path = pd.read_csv(os.path.join(path_to_files, 'challenge2_train_path_area.csv'))\n",
    "\n",
    "# Modify the DataFrame to include 'ct_path' and 'ct_slice' columns for 'train'\n",
    "challenge2_train_area_path[\"ct_path\"] = challenge2_train_area_path[\"path\"].apply(lambda x: \"/\".join(x.split(\"/\")[:-1]))\n",
    "challenge2_train_area_path[\"ct_slice\"] = challenge2_train_area_path[\"path\"].apply(lambda x: int(x.split(\"/\")[-1].split(\".\")[0]))\n",
    "\n",
    "# Sort the DataFrame by 'ct_path' and 'ct_slice'\n",
    "challenge2_train_area_path.sort_values(by=['ct_path', 'ct_slice'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d02219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file for 'valid' in challenge2\n",
    "challenge2_valid_area_path = pd.read_csv(os.path.join(path_to_files, 'challenge2_valid_path_area.csv'))\n",
    "\n",
    "# Modify the DataFrame to include 'ct_path' and 'ct_slice' columns for 'valid'\n",
    "challenge2_valid_area_path[\"ct_path\"] = challenge2_valid_area_path[\"path\"].apply(lambda x: \"/\".join(x.split(\"/\")[:-1]))\n",
    "challenge2_valid_area_path[\"ct_slice\"] = challenge2_valid_area_path[\"path\"].apply(lambda x: int(x.split(\"/\")[-1].split(\".\")[0]))\n",
    "\n",
    "# Sort the DataFrame by 'ct_path' and 'ct_slice'\n",
    "challenge2_valid_area_path.sort_values(by=['ct_path', 'ct_slice'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file for 'non_annotated' in challenge2\n",
    "challenge2_non_annotated_area_path = pd.read_csv(os.path.join(path_to_files, 'challenge2_non_annotated_path_area.csv'))\n",
    "\n",
    "# Modify the DataFrame to include 'ct_path' and 'ct_slice' columns for 'non_annotated'\n",
    "challenge2_non_annotated_area_path[\"ct_path\"] = challenge2_non_annotated_area_path[\"path\"].apply(lambda x: \"/\".join(x.split(\"/\")[:-1]))\n",
    "challenge2_non_annotated_area_path[\"ct_slice\"] = challenge2_non_annotated_area_path[\"path\"].apply(lambda x: int(x.split(\"/\")[-1].split(\".\")[0]))\n",
    "\n",
    "# Sort the DataFrame by 'ct_path' and 'ct_slice'\n",
    "challenge2_non_annotated_area_path.sort_values(by=['ct_path', 'ct_slice'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8298b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming challenge2_train_area_path is previously defined and has the 'ct_path' and 'area' columns\n",
    "challenge2_ct_path_list = challenge2_train_area_path[\"ct_path\"].unique()\n",
    "challenge2_train_dic = {}\n",
    "\n",
    "for i in tqdm(range(len(challenge2_ct_path_list))):\n",
    "    # Filter the DataFrame for the current CT path and reset the index\n",
    "    tmp_df = challenge2_train_area_path[challenge2_train_area_path[\"ct_path\"] == challenge2_ct_path_list[i]].reset_index(drop=True)\n",
    "    \n",
    "    # Assuming sum_max is a function you've defined elsewhere\n",
    "    challenge2_train_dic[challenge2_ct_path_list[i]] = list(sum_max(tmp_df[\"area\"].values, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1dadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_ct_path_list = challenge2_valid_area_path[\"ct_path\"].unique()\n",
    "challenge2_valid_dic = {}\n",
    "\n",
    "for i in tqdm(range(len(challenge2_ct_path_list))):\n",
    "    # Filter the DataFrame for the current CT path and reset the index\n",
    "    tmp_df = challenge2_valid_area_path[challenge2_valid_area_path[\"ct_path\"] == challenge2_ct_path_list[i]].reset_index(drop=True)\n",
    "    \n",
    "    # Assuming sum_max is a function you've defined elsewhere\n",
    "    challenge2_valid_dic[challenge2_ct_path_list[i]] = list(sum_max(tmp_df[\"area\"].values, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_ct_path_list = challenge2_non_annotated_area_path[\"ct_path\"].unique()\n",
    "challenge2_non_annotated_dic = {}\n",
    "\n",
    "for i in tqdm(range(len(challenge2_ct_path_list))):\n",
    "    # Filter the DataFrame for the current CT path and reset the index\n",
    "    tmp_df = challenge2_non_annotated_area_path[challenge2_non_annotated_area_path[\"ct_path\"] == challenge2_ct_path_list[i]].reset_index(drop=True)\n",
    "    \n",
    "    # Assuming sum_max is a function you've defined elsewhere\n",
    "    challenge2_non_annotated_dic[challenge2_ct_path_list[i]] = list(sum_max(tmp_df[\"area\"].values, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f40f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_files,'challenge2_valid_range.pickle'), 'wb') as handle:\n",
    "    pickle.dump(challenge2_valid_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624afbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_files,'challenge2_non_annotated_range.pickle'), 'wb') as handle:\n",
    "    pickle.dump(challenge2_non_annotated_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_files,'challenge2_train_range.pickle'), 'wb') as handle:\n",
    "    pickle.dump(challenge2_train_dic, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca4cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_train_scan_path_range = pd.DataFrame(list(challenge2_train_dic.items()), columns=['Path', 'Range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd94c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_valid_scan_path_range = pd.DataFrame(list(challenge2_valid_dic.items()), columns=['Path', 'Range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_non_annotated_scan_path_range = pd.DataFrame(list(challenge2_non_annotated_dic.items()), columns=['Path', 'Range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ba1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge1_train_scan_path_range = pd.DataFrame(list(challenge1_train_dic.items()), columns=['Path', 'Range'])\n",
    "challenge1_valid_scan_path_range = pd.DataFrame(list(challenge1_valid_dic.items()), columns=['Path', 'Range'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48ac1b",
   "metadata": {},
   "source": [
    "#### The following to label each CT Scan #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the 'train' data scenario\n",
    "challenge1_train_scan_path_range['Label'] = 0\n",
    "\n",
    "\n",
    "# challenge1_train_scan_path_range_label = challenge1_train_scan_path_range\n",
    "# However, to create a separate DataFrame for modifications, use .copy()\n",
    "challenge1_train_scan_path_range_label = challenge1_train_scan_path_range.copy()\n",
    "\n",
    "# Update 'Label' based on a condition in the copied or original DataFrame\n",
    "challenge1_train_scan_path_range_label['Label'] = challenge1_train_scan_path_range_label['Path'].apply(lambda x: 1 if '/positive' in x else 0)\n",
    "\n",
    "# For the 'valid' data scenario\n",
    "\n",
    "# Assuming challenge1_valid_scan_path_range is your original DataFrame for 'valid' data\n",
    "# Make a copy of the DataFrame to work with\n",
    "challenge1_valid_scan_path_range_label = challenge1_valid_scan_path_range.copy()\n",
    "\n",
    "# Initialize the 'Label' column to 0 in the new DataFrame\n",
    "challenge1_valid_scan_path_range_label['Label'] = 0\n",
    "\n",
    "# Use .apply() to update 'Label' based on a condition in the new DataFrame\n",
    "challenge1_valid_scan_path_range_label['Label'] = challenge1_valid_scan_path_range_label['Path'].apply(lambda x: 1 if '/positive' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c41f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge2_train_scan_path_range['Label'] = 0\n",
    "\n",
    "challenge2_train_scan_path_range_label=challenge2_train_scan_path_range\n",
    "# Use .apply() to update 'Label' based on a condition\n",
    "challenge2_train_scan_path_range_label['Label'] = challenge2_train_scan_path_range['Path'].apply(lambda x: 1 if '/cov_1' in x else 0)\n",
    "\n",
    "\n",
    "\n",
    "# Make a copy of the DataFrame to work with\n",
    "challenge2_valid_scan_path_range_label = challenge2_valid_scan_path_range.copy()\n",
    "\n",
    "# Initialize the 'Label' column to 0 in the new DataFrame\n",
    "challenge2_valid_scan_path_range_label['Label'] = 0\n",
    "\n",
    "# Use .apply() to update 'Label' based on a condition in the new DataFrame\n",
    "challenge2_valid_scan_path_range_label['Label'] = challenge2_valid_scan_path_range_label['Path'].apply(lambda x: 1 if '/cov_1' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc4274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f44c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30df9d3a",
   "metadata": {},
   "source": [
    "Saving the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1198aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge1_train_scan_path_range_label.to_csv(os.path.join(path_to_files,'la-challenge1_train_path_range_label.csv'))\n",
    "challenge1_valid_scan_path_range_label.to_csv(os.path.join(path_to_files,'la-challenge1_valid_path_range_label.csv'))\n",
    "\n",
    "challenge2_train_scan_path_range_label.to_csv(os.path.join(path_to_files,'la-challenge2_train_path_range_label.csv'))\n",
    "challenge2_valid_scan_path_range_label.to_csv(os.path.join(path_to_files,'la-challenge2_valid_scan_path_range_label.csv'))\n",
    "challenge2_non_annotated_scan_path_range.to_csv(os.path.join(path_to_files,'la-challenge2_non_annotated_scan_path_range.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe08700",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddac1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
